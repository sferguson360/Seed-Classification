# ğŸŒ¾ Seed Classification using OLS Regression

## ğŸ“Œ Overview

This project explores the use of **Ordinary Least Squares (OLS)** regression to classify three varieties of wheat seeds using a subset of features from the UCI Seeds dataset. While OLS is traditionally a regression method, it is evaluated here for its interpretability and statistical significance in feature selection for classification tasks.

---

## ğŸ“ Academic Context

This project was completed as part of the **Statistical Learning** course in the **Master of Data Science** program at the **Illinois Institute of Technology**. It demonstrates foundational modeling, hypothesis testing, and analysis techniques for supervised learning using real-world datasets.

---

## ğŸ“Š Dataset

* **Source**: [UCI Machine Learning Repository â€“ Seeds Dataset](https://doi.org/10.24432/C5H30K)
* **Samples**: 210 seed observations (70 per class)
* **Classes**: Kama, Rosa, Canadian
* **Features**:

  * Area (A)
  * Perimeter (P)
  * Compactness (C) = 4Ï€A / PÂ²
  * Length of Kernel
  * Width of Kernel
  * Asymmetry Coefficient
  * Length of Kernel Groove

The original labels (1 = Kama, 2 = Rosa, 3 = Canadian) were mapped to species names and one-hot encoded for regression-based classification.

---

## ğŸ““ Dataset Preparation

* **Header Fix**: Dataset lacked headers; corrected using `header=None`
* **Delimiter Issue**: Corrected tab misalignment using `delim_whitespace=True`
* **Target Encoding**: Integer labels were mapped to class names, then one-hot encoded
* **Train/Test Split**: 70/30 split using `train_test_split` from scikit-learn

---

## ğŸ¤¹ Feature Selection

Feature importance was first evaluated using a **Random Forest** model. Pairwise plots were also created to visually inspect class separability. Based on these, the two most influential featuresâ€”**Area** and **Length of Kernel Groove**â€”were selected for the primary OLS model.

---

## ğŸš€ Model Implementation

* **One-hot Encoding**: Converted categorical target into 3 binary columns
* **OLS Regression**: Built using `LinearRegression()` from scikit-learn
* **Prediction Logic**: The class with the highest predicted value among the three regression outputs was selected as the final class

---

## âœ… Results

* **Accuracy**: 92.06%
* **Coefficients**:

  * Length of Kernel Groove: **2.172**
  * Area: **0.4126**
* **Rosa**: Easiest to classify (Precision = 1.00, Recall = 0.96)
* **Kama**: Most misclassifications (Precision = 0.85, Recall = 0.89)
* **Statistical Significance**: p-values near zero for predictors; F-statistic p = **1.32e-22**

---

## ğŸ” Evaluation Insights

* OLS RÂ² value = 0.504 (low due to categorical output)
* Accuracy, precision, recall better indicators for this classification task
* Macro, micro, and weighted metrics were similar, showing no class imbalance bias

---

## âš ï¸ Limitations

* Small dataset (210 observations)
* Only two predictors used in main model
* Logistic regression would be more appropriate for classification
* Full 7-feature model (in code only) achieved 100% accuracy but lacks generalizability

---

## ğŸš€ Future Improvements

* Apply logistic regression or ensemble classifiers
* Use cross-validation
* Try 3â€“4 best predictors to balance simplicity and performance

---

## ğŸ’¬ Reflections

> I initially underestimated how much effort the written report would take, even though the code was completed weeks earlier. This project pushed me to think more critically about model interpretation and report clarity. Using JupyterLab with extensions like Spellcheck and Variable Inspector helped manage variables and structure. I also learned a lot about data cleaning (e.g., fixing misaligned tabs), one-hot encoding, and the importance of understanding dimensionality and formatting.

> This wasnâ€™t just about getting a working model, but about clearly communicating what the model means and how to assess its performance. That shift in mindset was a major learning moment for me.

---

## ğŸ§° Libraries Used

This project relies on the following Python libraries:

* **pandas**, **numpy** â€“ data wrangling and numerical operations
* **matplotlib**, **seaborn** â€“ data visualization
* **scikit-learn** â€“ modeling (LinearRegression, LogisticRegression, RandomForestClassifier), train-test splitting, evaluation metrics
* **statsmodels** â€“ statistical modeling (OLS regression, p-values, F-statistics)
* **JupyterLab Extensions** â€“ Spellcheck, Variable Inspector for workflow improvement

---

## ğŸ“š Citation

Charytanowicz, M., Niewczas, J., Kulczycki, P., Kowalski, P., & Åukasik, S. (2010). *Seeds Dataset*. UCI Machine Learning Repository. [https://doi.org/10.24432/C5H30K](https://doi.org/10.24432/C5H30K)

---


